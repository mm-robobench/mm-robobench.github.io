<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Are Multimodal LLMs Eligible as the “Brain” for Robotics?</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta name="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="900">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />
    <meta name="twitter:image" content="https://innermonologue.github.io/img/teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>MMRo: Are Multimodal LLMs Eligible as the “Brain” for Robotics?</br>
                <!--<small>
                    
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

    <li>Jinming Li<sup>1*</li> <li>Yichen Zhu<sup>2*</li> <li>Zhiyuan Xu<sup>2</li> <li>Jindong Gu<sup>3</li> <li>Ning Liu<sup>2</li> <li>Xin Liu<sup>4</li>
        <br><br><li>Minjie Zhu<sup>4</li> <li>Ran Cheng<sup>2</li> <li>Tao Sun <sup>2</li><li>Yaxin Peng <sup>1</li> <li>Feifei Feng<sup>2</li> <li>Jian Tang<sup>2</li>
                <br><br>
                    <!-- <a href="http://g.co/robotics"> -->
                    <!-- <img src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br> -->
                    <h5><sup>1</sup> Shanghai University, China</h5>
                    <h5><sup>2</sup> Midea Group, China</h5>
                    <h5><sup>3</sup> University of Oxford</h5>
                    <h5><sup>4</sup> East China Normal University</h5>
<!--                    <h5> * Equal contribution. This work was done during Minjie Zhu, Jinming Li, and Junjie Wen’s internship at Midea Group.</h5>-->
                </ul>
            </div>
        </div>


<div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    It is fundamentally challenging for robots to serve as useful assistants in human environments because this requires addressing a spectrum of sub-problems across robotics, including perception, language understanding, reasoning, and planning. The recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated their exceptional abilities in solving complex mathematical problems, mastering commonsense and abstract reasoning. This has led to the recent utilization of MLLMs as the "brain" in robotic systems, enabling these models to conduct high-level planning prior to triggering low-level control actions for task execution. However, it remains uncertain whether existing MLLMs are reliable in serving the brain role of robots. In this study, we introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo) benchmark, which tests the capability of MLLMs for robot applications. Specifically, we identify four essential capabilities — perception, task planning, visual reasoning, and safety measurement — that MLLMs must possess to qualify as the robot's central processing unit. We have developed several scenarios for each capability, resulting in a total of 14 metrics for evaluation. We present experimental results for various MLLMs, including both commercial and open-source models, to assess the performance of existing systems. Our findings indicate that no single model excels in all areas, suggesting that current MLLMs are not yet trustworthy enough to serve as the cognitive core for robots. The dataset is available at <a href="https://drive.google.com/open?id=1-1vRftbhobKwUFFo3_YxJ2eWoNuC4YFn&usp=drive_fs">Google Drive</a>.

                </p>
            
            </div>
        </div>





        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br>
                <h3>
                    Overview
                </h3>
    <p>
        Our work the first diagnostic benchmark specifically designed to systematically dissect and analyze the diverse failure modes of MLLMs (Multimodal Large Language Models) in robotics. MMRo includes approximately 26,175 meticulously crafted visual question-answer (VQA) pairs. We demonstrate some data samples convering 14 scenarios that are crucial for the application of MLLMs in robotics.
    </p>
                <div class="text-center">
                    <image src="img/dataste_view.png" width="80%">
                </div>



     <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br>
                <h3>
                    Overview
                </h3>
    <p>
        We evaluate 13 models, including open-sourced and commercial MLLMs, on MMRo. We show the experimental results on open-ended questions in the following.
    </p>
                <div class="text-center">
                    <image src="img/mmro_openended_all.png" width="100%">
                </div>
        </div>
    </div>

    <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Citation
                    </h3>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@article{li2024mmro,
    title={MMRo: Are Multimodal LLMs Eligible as the “Brain” for Robotics?},
    author={Li, Jinming and Zhu, Yichen and Xu, Zhiyuan and Gu, Jindong and Liu, Ning and others},
    year={2024}
    }</textarea>
                </div>
         </div>


</body>
</html>